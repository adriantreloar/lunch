import datetime
import itertools
from pathlib import Path

import pandas as pd

import pyarrow as pa
import pyarrow.flight
import json
import asyncio
import concurrent.futures as futures
from threading import Thread
import time

from src.lunch.mvcc.version import version_to_dict

from src.lunch.examples.setup_managers import model_manager, version_manager
from src.lunch.examples.insert_dimension_data import insert_dimension_data
from src.lunch.examples.save_fact import save_fact

from src.lunch.storage.persistence.local_file_columnar_fact_data_persistor import (
    LocalFileColumnarFactDataPersistor,
)
from src.lunch.storage.cache.null_fact_data_cache import NullFactDataCache
from src.lunch.storage.serialization.columnar_fact_data_serializer import (
    ColumnarFactDataSerializer,
)
from src.lunch.storage.fact_data_store import FactDataStore
from src.lunch.import_engine.fact_append_planner import FactAppendPlanner
from src.lunch.import_engine.fact_import_optimiser import FactImportOptimiser
from src.lunch.import_engine.fact_import_enactor import FactImportEnactor
from src.lunch.managers.cube_data_manager import CubeDataManager

from src.lunch.model.table_metadata import TableMetadata, TableMetadataTransformer

async def insert_fact_data_example():

    CSV_PATH = "/home/treloarja/PycharmProjects/lunch/example_output/fact1.csv"

    await insert_dimension_data()
    await save_fact()

    data = [
        {
            "department_thing" : f"A Thing {j}",
            "thing 2": f"Time {i}",
            "sales value": j+0.10
        } for i, j in itertools.product(range(1000), repeat=2)
    ]

    df_data = pd.DataFrame(data=data)

    print(df_data)

    df_data.to_csv(CSV_PATH, index=False)

    # TODO - create the map between data columns and where they will go in the fact star schema model
    # Source should be autogenerated from df_data - we should have a function to do this
    # Target - should be a model pulled from model_manager using get_star_schema_model_by_fact_name("Sales")

    column_mapping = [{"source": ["department_thing"],
                       "target": ["Department", "thing1"]
                       },
                      {"source": ["thing 2"],
                       "target": ["Time", "thing2"],
                       },
                      {"source": ["sales value"],
                       # A measure target puts the original value into 'value'
                       # amd the measure id into 'measure dimension'
                       "measure target": ["measures", "sales"],
                       }
                      ]

    source_metadata = TableMetadata(column_names=list(df_data.columns),
                                    column_types=list(df_data.dtypes),
                                    length=df_data.shape[0])

    fact_data_persistor = LocalFileColumnarFactDataPersistor(
        directory=Path(
            "/home/treloarja/PycharmProjects/lunch/example_output/fact"
        )
    )
    fact_data_cache = NullFactDataCache()
    fact_data_serializer = ColumnarFactDataSerializer(
        persistor=fact_data_persistor
    )

    fact_data_storage = FactDataStore(
        serializer=fact_data_serializer, cache=fact_data_cache
    )

    fact_append_planner = FactAppendPlanner()
    fact_import_optimiser = FactImportOptimiser(
        fact_append_planner=fact_append_planner,
        fact_data_store=fact_data_storage,
        model_manager=model_manager,
    )
    fact_import_enactor = FactImportEnactor()

    cube_data_manager = CubeDataManager(
        model_manager=model_manager,
        fact_data_store=fact_data_storage,
        fact_import_optimiser=fact_import_optimiser,
        fact_import_enactor=fact_import_enactor,
    )

    start_time = datetime.datetime.now()
    client = pa.flight.connect("grpc://0.0.0.0:8819")

    call_options = pa.flight.FlightCallOptions(timeout=None)

    async with version_manager.read_version() as read_version:
        async with version_manager.write_reference_data_version(
                read_version=read_version
        ) as write_version:
            write_version_sales_star_schema = await model_manager.get_star_schema_model_by_fact_name(name="Sales",
                                                                                                     version=write_version)
            # TODO add parameters,
            #  Regenerate this from examples/insert_fact_data.py
            #  TableMetadata(column_names=StrPVector(['department_id', 'thing 2', 'sales value']), column_types=DtypePVector([dtype('int64'), dtype('int64'), dtype('float64')]), length=3)
            #  {'source': ['department_id'], 'target': ['Department', 'id_']}
            #  {'source': ['thing 2'], 'target': ['Time', 'thing 2']}
            #  {'source': ['sales value'], 'measure target': ['measures', 'sales']}

            source_schema = source_metadata.serialize()
            print(source_schema)

            command_dict = {"command": "import_fact_from_csv",
                            "parameters":
                                {"read_version": version_to_dict(read_version),
                                 "write_version": version_to_dict(write_version),
                                 "read_version_target_schema": None,
                                 "write_version_target_schema": write_version_sales_star_schema.serialize(),
                                 "source_schema": source_schema,
                                 "column_mappings": column_mapping,
                                 "csv_file_path": CSV_PATH,
                                 # TODO: csv_has_headers feels like it should sort of leave near the source_schema
                                 "csv_has_headers": True
                                 }
                            }
            command = json.dumps(command_dict, default=str).encode("utf8")
            print(command_dict)
            print()

            upload_descriptor = pa.flight.FlightDescriptor.for_command(command)
            writer, reader = client.do_put(descriptor=upload_descriptor,
                                           schema = pa.schema(fields=[]),
                                           options=call_options)

            print(writer)
            print(reader)

            # Writer thread
            with futures.ThreadPoolExecutor(max_workers=2) as executor:
                print("submitting write thread")
                write_future = executor.submit(_write_batches, writer, pa.schema(fields=[]))
                print("submitted write thread")

                def read_it_all(reader):
                    result = reader.read()
                    return result

                print("submitting read thread")
                read_future = executor.submit(read_it_all, reader)
                print("submitted read thread")

                print("joining worker threads", datetime.datetime.utcnow())
                print(write_future.result(timeout=10))

                print(read_future.result(timeout=10))
                print("joined worker thread", datetime.datetime.utcnow())

                writer.close()

        print(f"READ  TIME {datetime.datetime.utcnow()-start_time}")
        print(f"FIN", datetime.datetime.utcnow())


def _write_batches(writer, schema):
    writer.done_writing()

    return 0 #total_rows

# And run it
if __name__ == "__main__":
    asyncio.run(insert_fact_data_example())
